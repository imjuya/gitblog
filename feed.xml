<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom"><id>https://github.com/imjuya/gitblog</id><title>RSS feed of imjuya's gitblog</title><updated>2026-02-21T06:50:51.694320+00:00</updated><link href="https://github.com/imjuya/gitblog"/><link href="https://raw.githubusercontent.com/imjuya/gitblog/master/feed.xml" rel="self"/><generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator><entry><id>https://github.com/imjuya/gitblog/issues/6</id><title>2026-02-21</title><updated>2026-02-21T06:50:51.989438+00:00</updated><content type="html"><![CDATA[<p><img src="http://testtttt.oss-cn-guangzhou.aliyuncs.com/imagehub/20260221/20260221084518976101edba_cover_46a9.png" alt="" /></p>
<h1>AI 早报 2026-02-21</h1>
<p><strong>视频版</strong>：<a href="https://www.youtube.com/watch?v=GddxDudZ_bE">YouTube</a> ｜ <a href="https://www.bilibili.com/video/BV1XAf7BnEy6">哔哩哔哩</a></p>
<h2>概览</h2>
<h3>开发生态</h3>
<ul>
<li>Anthropic 员工回应 Claude Agent SDK 及 OAuth 令牌争议 <a href="https://x.com/trq212/status/2024212380142752025">↗</a> <code>#1</code></li>
<li>Claude Code 桌面版发布重大更新 <a href="https://x.com/claudeai/status/2024937960572104707">↗</a> <code>#2</code></li>
<li>Anthropic 发布 Claude Code Security <a href="https://www.anthropic.com/news/claude-code-security">↗</a> <code>#3</code></li>
<li>GPT-5.3-Codex-Spark 升级速率超 1200 tokens/s <a href="https://x.com/thsottiaux/status/2024947946849186064">↗</a> <code>#4</code></li>
<li>Google CLI 开启 3.1 Pro 访问权限 <a href="https://x.com/geminicli/status/2024967271681233356">↗</a> <code>#5</code></li>
<li>Ollama 发布新版原生集成 Cline 与 Pi <a href="https://x.com/ollama/status/2024987888870416673">↗</a> <code>#6</code></li>
</ul>
<h3>产品应用</h3>
<ul>
<li>OpenAI 扩充 ChatGPT Thinking 模式上下文窗口 <a href="https://x.com/trq212/status/2024212380142752025">↗</a> <code>#7</code></li>
</ul>
<h3>行业动态</h3>
<ul>
<li>智谱 MiniMax 市值接连超越快手京东 <a href="https://mp.weixin.qq.com/s/neThU7OwEOM7S_TSSroI4A">↗</a> <code>#8</code></li>
<li>GGML 团队加入 Hugging Face <a href="https://huggingface.co/blog/ggml-joins-hf">↗</a> <code>#9</code></li>
</ul>
<h3>技术与洞察</h3>
<ul>
<li>Karpathy 称传统 App Store 模式将过时 <a href="https://x.com/karpathy/status/2024583544157458452">↗</a> <code>#10</code></li>
</ul>
<h3>前瞻与传闻</h3>
<ul>
<li>OpenAI 或将推出 ChatGPT Pro Lite 订阅 <a href="https://x.com/btibor91/status/2024992285591818329">↗</a> <code>#11</code></li>
<li>Google DeepMind 宣布将发布 Gemma 新模型 <code>#12</code></li>
<li>传 OpenAI 打造首款摄像头 AI 智能音箱 <a href="https://www.theinformation.com/articles/inside-openai-team-developing-ai-devices">↗</a> <code>#13</code></li>
<li>OpenAI 传获软银领衔百亿注资 <a href="https://the-decoder.com/nvidia-reportedly-set-to-invest-30-billion-in-openai/">↗</a> <code>#14</code></li>
</ul>
<hr />
<h2><a href="https://x.com/trq212/status/2024212380142752025">Anthropic 员工回应 Claude Agent SDK 及 OAuth 令牌争议</a> <code>#1</code></h2>
<blockquote>
<p><strong>Anthropic</strong> 员工回应了关于 <code>Claude Agent SDK</code> 及 <code>OAuth</code> 令牌使用的争议，称此前的文档变更仅为措辞调整，鼓励开发者使用 <code>Agent SDK</code> 进行本地实验，但如果基于该 <code>SDK</code> 构建商业产品，则必须通过 <code>API Key</code> 进行调用。</p>
</blockquote>
<p>针对<code>Claude Agent SDK</code>及<code>OAuth令牌</code>使用政策的争议，<strong>Anthropic</strong>员工<strong>Thariq</strong>近日澄清，引发混淆的文档变更仅为措辞调整，并未改变<code>Agent SDK</code>及<code>MAX</code>订阅的使用方式。官方立场鼓励使用<code>Agent SDK</code>进行本地开发与实验，但若基于其构建商业产品，则需使用<code>API Key</code>。其表示将优化文档表述以消除误解。</p>
<p><img src="https://cdn.jsdelivr.net/gh/imjuya/picx-images-hosting@master/imagehub/aidaily/46a9251e-3ea4-4df4-a688-cf90f543a579/5f83d37d-0684-4302-9c27-9590d079fb04/m001.png" alt="" /></p>
<p>相关链接：</p>
<ul>
<li><a href="https://x.com/trq212/status/2024212380142752025">https://x.com/trq212/status/2024212380142752025</a></li>
</ul>
<hr />
<h2><a href="https://x.com/claudeai/status/2024937960572104707">Claude Code 桌面版发布重大更新</a> <code>#2</code></h2>
<blockquote>
<p><code>Claude Code</code> 桌面版迎来重大更新，新增 <code>Server Previews</code>、<code>Local Code Review</code>、<code>PR Monitoring</code> 及 <code>Session Mobility</code> <strong>四大</strong>核心功能。<code>Claude</code> 现在能直接在桌面端启动服务器预览应用，自动审查<code>代码</code>中的<code>Bug</code>，并监控<code>CI</code>状态，实现自动修复与合并。</p>
</blockquote>
<p><strong>Claude</strong> 官方宣布 <code>Claude Code</code> 桌面版迎来重大更新，新增 <code>Server previews</code>、<code>Local code review</code>、<code>PR monitoring</code> 及 <code>Session mobility</code> 核心功能。更新后，<strong>Claude</strong> 可在桌面端直接启动开发服务器并预览应用，结合日志与截图进行迭代；代码提交前支持行内 Bug 审查；PR 流程中具备 <code>Auto-fix</code> 和 <code>Auto-merge</code> 能力，可自动修复故障并在通过检查后合并代码。此外，<code>Session mobility</code> 实现了 <code>CLI</code> 与桌面、云端及移动端间的会话无缝迁移。</p>
<p><img src="https://cdn.jsdelivr.net/gh/imjuya/picx-images-hosting@master/imagehub/aidaily/46a9251e-3ea4-4df4-a688-cf90f543a579/cd6a6d9a-d92c-4a06-92ea-cb636b11effd/m001.gif" alt="" /></p>
<p>相关链接：</p>
<ul>
<li><a href="https://x.com/claudeai/status/2024937960572104707">https://x.com/claudeai/status/2024937960572104707</a></li>
</ul>
<hr />
<h2><a href="https://www.anthropic.com/news/claude-code-security">Anthropic 发布 Claude Code Security</a> <code>#3</code></h2>
<blockquote>
<p><strong>Anthropic</strong> 推出了集成在 <code>Claude Code</code> 中的新功能 <code>Claude Code Security</code>，该功能由 <code>Claude Opus 4.6</code> 驱动，能像人类安全专家一样对代码进行推理，从而发现传统工具难以捕捉的业务逻辑漏洞。<strong>Enterprise</strong> 和 <strong>Team</strong> 客户以及开源项目维护者现可申请试用。</p>
</blockquote>
<p><strong>Anthropic</strong> 官方宣布推出 <code>Claude Code Security</code>，现处于有限研究预览阶段。该功能集成在 <code>Claude Code (Web)</code> 中，由 <code>Claude Opus 4.6</code> 驱动。不同于依赖<code>模式匹配</code>的传统<code>静态分析工具</code>，该工具能像人类一样推理代码，通过理解组件交互和数据流向，发现<code>业务逻辑缺陷</code>等<code>复杂漏洞</code>。官方数据显示，该模型已协助团队在开源代码中发现超过 <strong>500</strong> 个隐藏漏洞。所有发现均经过多阶段验证和评级，修复需经人工批准。目前，<strong>Enterprise</strong> 和 <strong>Team</strong> 客户及开源项目维护者均可申请访问。</p>
<p><img src="https://cdn.jsdelivr.net/gh/imjuya/picx-images-hosting@master/imagehub/aidaily/46a9251e-3ea4-4df4-a688-cf90f543a579/6c68bcc6-1913-48d7-80c1-06f7ac6ad98f/m001.gif" alt="" /></p>
<p>相关链接：</p>
<ul>
<li><a href="https://www.anthropic.com/news/claude-code-security">https://www.anthropic.com/news/claude-code-security</a></li>
</ul>
<hr />
<h2><a href="https://x.com/thsottiaux/status/2024947946849186064">GPT-5.3-Codex-Spark 升级速率超 1200 tokens/s</a> <code>#4</code></h2>
<blockquote>
<p><strong>OpenAI</strong> 宣布 <code>GPT-5.3-Codex-Spark</code> 模型速度得到提升，突破每秒 <strong>1200</strong> 个 <code>tokens</code>，后续将推出更多全局性速度优化。</p>
</blockquote>
<p><code>GPT-5.3-Codex-Spark</code> 官方宣布该模型已完成性能升级，速度提升约 <strong>30%</strong>，当前服务速度已超 <strong>1200</strong> tokens/s。此外，官方预告未来将在全范围内推出更多速度优化计划。</p>
<p><img src="https://cdn.jsdelivr.net/gh/imjuya/picx-images-hosting@master/imagehub/aidaily/46a9251e-3ea4-4df4-a688-cf90f543a579/5391a66f-9c5d-4547-8fb6-a7b502695166/m001.png" alt="" /></p>
<p>相关链接：</p>
<ul>
<li><a href="https://x.com/thsottiaux/status/2024947946849186064">https://x.com/thsottiaux/status/2024947946849186064</a></li>
</ul>
<hr />
<h2><a href="https://x.com/geminicli/status/2024967271681233356">Google CLI 开启 3.1 Pro 访问权限</a> <code>#5</code></h2>
<blockquote>
<p><code>Gemini CLI</code> 目前已向 <strong>AI Ultra</strong> 和 <strong>Workspace Ultra</strong> 账户开放 <code>Gemini 3.1 Pro</code> 访问权限。官方将根据系统容量逐步扩大访问范围。</p>
</blockquote>
<p><strong>Gemini CLI</strong> 团队正在逐步推送 <code>Gemini 3.1 Pro</code> 访问权限。目前获权用户包括：通过 <strong>Google</strong> 登录的 <strong>AI Ultra</strong> 和 <strong>Workspace Ultra</strong> 账户、拥有该模型权限的 <strong>AI Studio</strong> 及 <strong>Vertex AI</strong> 项目 <code>API Key</code> 持有者，以及小部分其他登录用户。用户可在 <code>CLI</code> 中输入 <code>/model</code> 命令验证状态。官方表示将根据系统容量逐步扩大范围。</p>
<p><img src="https://cdn.jsdelivr.net/gh/imjuya/picx-images-hosting@master/imagehub/aidaily/46a9251e-3ea4-4df4-a688-cf90f543a579/495b9cf1-b67e-4a5c-b1a9-c04f1a9f63b0/m001.png" alt="" /></p>
<p>相关链接：</p>
<ul>
<li><a href="https://x.com/geminicli/status/2024967271681233356">https://x.com/geminicli/status/2024967271681233356</a></li>
<li><a href="https://github.com/google-gemini/gemini-cli/discussions/19724">https://github.com/google-gemini/gemini-cli/discussions/19724</a></li>
</ul>
<hr />
<h2><a href="https://x.com/ollama/status/2024987888870416673">Ollama 发布新版原生集成 Cline 与 Pi</a> <code>#6</code></h2>
<blockquote>
<p><strong>Ollama</strong> 发布新版，集成了 <code>Cline</code> 和 <code>Pi</code>，用户通过指令 <code>ollama launch cline</code> 或 <code>ollama launch pi</code> 即可开箱即用。</p>
</blockquote>
<p><strong>Ollama</strong> 发布 <strong>0.16.3</strong> 版本，新增对 <strong>Cline</strong> 和 <strong>Pi</strong> 的开箱即用集成支持，用户可通过 <code>ollama launch</code> 指令直接启动相应服务。<strong>Cline</strong> 作者 <strong>Saoud Rizwan</strong> 对此表示，尽管目前尚处早期，但未来笔记本电脑运行的开源模型将足以完成大部分工作，很高兴与 <strong>Ollama</strong> 合作推动这一愿景成为现实。</p>
<p><img src="https://cdn.jsdelivr.net/gh/imjuya/picx-images-hosting@master/imagehub/aidaily/46a9251e-3ea4-4df4-a688-cf90f543a579/f33b7761-2fb5-429e-83e9-28f1e22a3e96/m001.png" alt="" /></p>
<p>相关链接：</p>
<ul>
<li><a href="https://x.com/ollama/status/2024987888870416673">https://x.com/ollama/status/2024987888870416673</a></li>
</ul>
<hr />
<h2><a href="https://x.com/trq212/status/2024212380142752025">OpenAI 扩充 ChatGPT Thinking 模式上下文窗口</a> <code>#7</code></h2>
<blockquote>
<p><strong>ChatGPT</strong> 更新日志显示，<strong>ChatGPT</strong> 的 <code>Thinking 模式</code> <code>上下文窗口</code> 已扩容至 <strong>256k</strong> <code>tokens</code>。</p>
</blockquote>
<p>根据 <strong>ChatGPT</strong> 更新日志，<strong>ChatGPT</strong> 的 <code>Thinking 模式</code>上下文窗口容量已扩充。当用户手动启用该模式时，其总上下文窗口从此前的 <strong>196k tokens</strong> 提升至 <strong>256k tokens</strong>。新版本的具体配置为 <strong>128k tokens</strong> 的输入限制和 <strong>128k tokens</strong> 的最大输出限制。</p>
<p><img src="https://cdn.jsdelivr.net/gh/imjuya/picx-images-hosting@master/imagehub/aidaily/46a9251e-3ea4-4df4-a688-cf90f543a579/366a06ca-7075-43cb-991b-9cc0217ac475/m001.png" alt="" /></p>
<p>相关链接：</p>
<ul>
<li><a href="https://x.com/trq212/status/2024212380142752025">https://x.com/trq212/status/2024212380142752025</a></li>
</ul>
<hr />
<h2><a href="https://mp.weixin.qq.com/s/neThU7OwEOM7S_TSSroI4A">智谱 MiniMax 市值接连超越快手京东</a> <code>#8</code></h2>
<blockquote>
<p>据报道，<strong>智谱</strong>与 <strong>MiniMax</strong> 港股股价大涨并创新高，市值已连超<strong>携程</strong>、<strong>快手</strong>及<strong>京东</strong>。</p>
</blockquote>
<p>据报道，大模型公司<strong>智谱</strong>与 <strong>MiniMax</strong> 近期港股股价大幅上涨，创上市以来新高。两家公司市值已接连超越<strong>携程</strong>、<strong>快手</strong>及<strong>京东</strong>，并逼近<strong>泡泡玛特</strong>与<strong>百度</strong>。</p>
<p>相关链接：</p>
<ul>
<li><a href="https://mp.weixin.qq.com/s/neThU7OwEOM7S_TSSroI4A">https://mp.weixin.qq.com/s/neThU7OwEOM7S_TSSroI4A</a></li>
</ul>
<hr />
<h2><a href="https://huggingface.co/blog/ggml-joins-hf">GGML 团队加入 Hugging Face</a> <code>#9</code></h2>
<blockquote>
<p><strong>Hugging Face</strong> 宣布 <strong>GGML</strong> 组织及其创始人 <strong>Georgi Gerganov</strong> 团队正式加入 <strong>Hugging Face</strong>，双方将结合 <code>llama.cpp</code> 在本地推理的优势与 <code>Transformers</code> 的模型定义能力，共同推动 <code>Local AI</code> 发展。</p>
</blockquote>
<p><strong>Hugging Face</strong> 官方宣布，<strong>GGML</strong> 组织及创始人 <strong>Georgi Gerganov</strong> 团队正式加入。双方将结合 <code>llama.cpp</code> 本地推理优势与 <code>transformers</code> 模型定义能力，推动 Local AI 发展。官方确认，原团队保留对 <code>llama.cpp</code> 的 <strong>100%</strong> 技术决策权与社区领导权，项目维持开源模式。未来将致力于实现“一键式”模型部署与优化 <strong>GGML</strong> 用户体验，使本地推理成为云端的有力替代。</p>
<p><img src="https://cdn.jsdelivr.net/gh/imjuya/picx-images-hosting@master/imagehub/aidaily/46a9251e-3ea4-4df4-a688-cf90f543a579/df4ae420-98e2-4f12-a3ff-b44d303a1917/m001.png" alt="" /></p>
<p>相关链接：</p>
<ul>
<li><a href="https://huggingface.co/blog/ggml-joins-hf">https://huggingface.co/blog/ggml-joins-hf</a></li>
</ul>
<hr />
<h2><a href="https://x.com/karpathy/status/2024583544157458452">Karpathy 称传统 App Store 模式将过时</a> <code>#10</code></h2>
<blockquote>
<p><strong>Andrej Karpathy</strong> 展示了他利用 <code>Claude</code> 仅用 <strong>1 小时</strong>就构建出的定制化心肺训练仪表盘，以此预言传统的 <strong>App Store</strong> 模式将过时，未来将由 <code>LLM Agent</code> 即时生成满足特定长尾需求的软件。</p>
</blockquote>
<p><strong>Andrej Karpathy</strong>通过构建定制化心肺训练追踪仪表盘，展示了他对未来“高度定制化软件”的设想。为执行一项为期<strong>8周</strong>的计划，旨在通过<code>Zone 2</code>训练与<code>HIIT</code>将静息心率从<strong>50</strong>降至<strong>45</strong>，他利用<code>Claude</code>在<strong>1小时</strong>内编写了一个约<strong>300行代码</strong>的<code>Web应用</code>。该应用通过<code>逆向工程</code><strong>Woodway</strong>跑步机云端<code>API</code>获取原始数据，尽管过程中出现单位换算和日历匹配错误需人工修复，但他认为这相比于<strong>2年前</strong>完成同样工作所需的<strong>10小时</strong>，效率已大幅提升。</p>
<p><strong>Karpathy</strong>指出，传统<strong>App Store</strong>模式正变得过时，因为针对高度特定的长尾需求，<code>LLM Agent</code>能够即时生成完全贴合需求的定制应用。他认为当前行业进展缓慢，<strong>99%<strong>的产品本质上作为传感器，却仍只提供人类可读的文档和<code>前端</code>，缺乏<code>AI原生</code>的<code>CLI</code>和<code>API</code>，迫使Agent进行<code>逆向工程</code>。他展望未来，行业应重构为具备<code>Agent原生工效学</code>的传感器与<code>执行器</code>服务，通过“<code>LLM胶水</code>”进行编排，并借助拥有大量个人上下文的<code>AI</code>，最终实现仅需</strong>1分钟</strong>即可构建高度定制、临时化应用程序的目标。</p>
<p><img src="https://cdn.jsdelivr.net/gh/imjuya/picx-images-hosting@master/imagehub/aidaily/46a9251e-3ea4-4df4-a688-cf90f543a579/98e67eea-963d-4845-aff0-23424da380c2/m001.png" alt="" /></p>
<p>相关链接：</p>
<ul>
<li><a href="https://x.com/karpathy/status/2024583544157458452">https://x.com/karpathy/status/2024583544157458452</a></li>
</ul>
<hr />
<h2><a href="https://x.com/btibor91/status/2024992285591818329">OpenAI 或将推出 ChatGPT Pro Lite 订阅</a> <code>#11</code></h2>
<blockquote>
<p>开发者<strong>Tibor Blaho</strong>在<code>ChatGPT</code>网页代码中发现了名为<code>ChatGPT Pro Lite</code>的新订阅层级，外界推测定价可能在<strong>50</strong>或<strong>100美元</strong>，但目前<strong>OpenAI</strong>官方尚未确认该消息。</p>
</blockquote>
<p>据开发者 <strong>Tibor Blaho</strong> 发现， <strong>ChatGPT</strong> 网页应用代码中出现了对“ <code>ChatGPT Pro Lite</code> ”计划的引用。此举引发了社区关于 <strong>OpenAI</strong> 或将推出新订阅层级的猜测，据推测其定价可能在 <strong>50至100美元</strong>。目前，官方尚未对该计划的存在、功能详情或上线时间做出任何确认。所有信息均源自代码线索，相关讨论仅为推测。</p>
<p><img src="https://cdn.jsdelivr.net/gh/imjuya/picx-images-hosting@master/imagehub/aidaily/46a9251e-3ea4-4df4-a688-cf90f543a579/43049b7c-9410-4e2c-a9ef-852f5c62f26b/m001.png" alt="" /></p>
<p>相关链接：</p>
<ul>
<li><a href="https://x.com/btibor91/status/2024992285591818329">https://x.com/btibor91/status/2024992285591818329</a></li>
</ul>
<hr />
<h2>Google DeepMind 宣布将发布 Gemma 新模型 <code>#12</code></h2>
<blockquote>
<p><strong>Google DeepMind</strong>联合创始人<strong>Demis Hassabis</strong>透露，即将发布针对<code>边缘设备</code>进行优化的<code>开源模型``Gemma</code>新版本。</p>
</blockquote>
<p>据社交媒体信息，<strong>Google DeepMind</strong>联合创始人<strong>Demis Hassabis</strong>近期在印度透露，公司将“很快”发布开源模型<code>Gemma</code>的新版本。<strong>Hassabis</strong>强调，新版本将针对<code>边缘设备</code>进行优化，具备强大性能，旨在为<code>资源受限环境</code>提供支持，拓展<code>终端</code>及<code>边缘计算</code>场景的能力边界。</p>
<hr />
<h2><a href="https://www.theinformation.com/articles/inside-openai-team-developing-ai-devices">传 OpenAI 打造首款摄像头 AI 智能音箱</a> <code>#13</code></h2>
<blockquote>
<p>据报道，<strong>OpenAI</strong>正研发一款配备摄像头的智能音箱作为首款<code>AI硬件</code>。该设备能通过<code>视觉感知</code>环境与用户面部，提供主动建议及<code>人脸识别</code>购物功能，预计售价<strong>200至300美元</strong>，最早<strong>2027年初</strong>上市。</p>
</blockquote>
<p>据报道，<strong>OpenAI</strong>正与前苹果设计师<strong>Jony Ive</strong>合作，组建一支超<strong>200</strong>人的团队研发<code>AI</code>硬件设备。其首款产品或为一款带摄像头的智能音箱，该设备可通过摄像头感知用户及环境，内置类<code>Face ID</code>的面部识别以支持交互，并能主动提供建议。产品售价预计<strong>200</strong>至<strong>300</strong>美元，最早或于<strong>2027年初</strong>上市。<strong>OpenAI</strong>还在探索智能灯、<code>AI</code>眼镜及耳机等产品，但这些项目尚处早期，存在被取消可能，预计<strong>2028年</strong>或更晚面世。</p>
<p>相关链接：</p>
<ul>
<li><a href="https://www.theinformation.com/articles/inside-openai-team-developing-ai-devices">https://www.theinformation.com/articles/inside-openai-team-developing-ai-devices</a></li>
<li><a href="https://the-decoder.com/openai-is-building-a-200-to-300-smart-speaker-that-tells-you-when-to-go-to-bed/">https://the-decoder.com/openai-is-building-a-200-to-300-smart-speaker-that-tells-you-when-to-go-to-bed/</a></li>
</ul>
<hr />
<h2><a href="https://the-decoder.com/nvidia-reportedly-set-to-invest-30-billion-in-openai/">OpenAI 传获软银领衔百亿注资</a> <code>#14</code></h2>
<blockquote>
<p>据多家媒体报道，<strong>OpenAI</strong> 正在推进新一轮巨额融资，<strong>软银</strong>预计注资 <strong>300 亿美元</strong>。此外，<strong>路透社</strong>消息称，<strong>英伟达</strong>也计划投资 <strong>300 亿美元</strong>，<strong>亚马逊</strong>可能参与其中。</p>
</blockquote>
<p>据多家媒体报道，<strong>OpenAI</strong>正在推进新一轮融资。<strong>The Information</strong>援引消息称，<strong>软银</strong>预计将作为锚定投资者注资<strong>300亿美元</strong>，<strong>亚马逊</strong>和<strong>英伟达</strong>也可能参与投资，金额或达<strong>数百亿美元</strong>。</p>
<p>针对<strong>英伟达</strong>的投资动向，<strong>路透社</strong>援引知情人士透露，<strong>英伟达</strong>也计划向<strong>OpenAI</strong>投资<strong>300亿美元</strong>。目前，上述投资计划均属市场传闻，官方尚未发布正式公告。</p>
<p>相关链接：</p>
<ul>
<li><a href="https://the-decoder.com/nvidia-reportedly-set-to-invest-30-billion-in-openai/">https://the-decoder.com/nvidia-reportedly-set-to-invest-30-billion-in-openai/</a></li>
<li><a href="https://x.com/theinformation/status/2024865405458661503">https://x.com/theinformation/status/2024865405458661503</a></li>
</ul>
<hr />
<p><strong>提示</strong>：内容由AI辅助创作，可能存在<strong>幻觉</strong>和<strong>错误</strong>。</p>
<p>作者<code>橘鸦Juya</code>，视频版在同名<strong>哔哩哔哩</strong>。欢迎<strong>点赞、关注、分享</strong>。</p>
]]></content><link href="https://github.com/imjuya/gitblog/issues/6"/><published>2026-02-21T06:50:35+00:00</published></entry><entry><id>https://github.com/imjuya/gitblog/issues/5</id><title>2026-02-18</title><updated>2026-02-21T06:50:52.152540+00:00</updated><content type="html"><![CDATA[<p><img src="http://testtttt.oss-cn-guangzhou.aliyuncs.com/imagehub/20260218/202602180850442485579dde_cover_e48a.jpg" alt="" /></p>
<h1>AI 早报 2026-02-18</h1>
<p><strong>视频版</strong>：<a href="https://www.youtube.com/watch?v=8jAigWfpDKU">YouTube</a> ｜ <a href="https://www.bilibili.com/video/BV1uAZDBiEKF">哔哩哔哩</a></p>
<h2>概览</h2>
<h3>精选</h3>
<ul>
<li>Anthropic 发布 Claude Sonnet 4.6 <code>#1</code></li>
<li>xAI 上线 Grok 4.20 测试版 <code>#2</code></li>
<li>NotebookLM推出幻灯片Prompt修订与PPTX导出 <code>#3</code></li>
</ul>
<h3>模型发布</h3>
<ul>
<li>蚂蚁集团开源Ming-omni-tts音频生成模型 <code>#4</code></li>
<li>Cohere Labs发布Tiny Aya多语言模型 <code>#5</code></li>
<li>字节跳动研究团队开源 BitDance 多模态模型 <code>#6</code></li>
</ul>
<h3>开发生态</h3>
<ul>
<li>Cursor 发布 2.5 版本更新，推出插件市场 <code>#7</code></li>
<li>OpenAI修复GPT-5.3-Codex请求重定向问题 <code>#8</code></li>
<li>Cerebras下调部分免费层级的推理额度 <code>#9</code></li>
<li>Intelligent Internet 开源多Agent协作系统 Common Ground Core <code>#10</code></li>
</ul>
<h3>行业动态</h3>
<ul>
<li>Nerve加入OpenAI构建ChatGPT搜索 <code>#11</code></li>
<li>传 Moonshot AI 完成7亿美元融资 <code>#12</code></li>
</ul>
<hr />
<h2><a href="https://www.anthropic.com/news/claude-sonnet-4-6">Anthropic 发布 Claude Sonnet 4.6</a> <code>#1</code></h2>
<blockquote>
<p><strong>Anthropic</strong> 正式发布了 <code>Claude Sonnet 4.6</code> 模型。该模型在编程、长上下文推理及 <code>Agent</code> 规划能力上全面升级，并支持 <strong>100 万</strong> <code>token</code> 上下文。同步推出的还有改进版网页搜索工具，在提升准确率的同时大幅降低了 <code>Token</code> 消耗。目前，<code>Sonnet 4.6</code> 已上线 <code>API</code> 及各类AI应用，价格与上一代保持一致，免费版用户现已可在<strong>Claude</strong>体验。</p>
</blockquote>
<p><strong>Anthropic</strong> 正式发布 <code>Claude Sonnet 4.6</code>，官方称其为迄今最强的 <code>Sonnet</code> 模型。该模型在编程、长上下文推理、<code>Agent</code> 规划、知识工作及设计等领域全面升级，并提供支持 <strong>100 万</strong> token 的上下文窗口（<code>Beta版</code>）。价格维持每百万 token 输入 <strong>3</strong> 美元、输出 <strong>15</strong> 美元不变。</p>
<p>性能提升显著。在编程方面，根据 <code>Claude Code</code> 的早期测试，约 <strong>70%</strong> 的开发者更偏好 <code>Sonnet 4.6</code> 而非上代模型，<strong>59%</strong> 的用户选择它而非旗舰 <code>Opus 4.5</code>。用户反馈其在修改代码前能更有效阅读上下文，并减少“偷懒”行为。在计算机使用能力上，<code>OSWorld</code> 基准测试得分从 <strong>14.0%</strong> 大幅提升至 <strong>72.5%</strong>，能更有效地处理复杂电子表格和多步网页表单任务。据外部评估，<code>Sonnet 4.6</code> 在部分真实工作任务基准上略微优于 <code>Opus 4.6</code>。</p>
<p><strong>Anthropic</strong> 同步推出改进版 <code>Web Search</code> 和 <code>Web Fetch</code> 工具，通过 <code>代码执行</code> 对搜索结果进行动态过滤，官方数据显示平均准确率提升 <strong>11%</strong>，输入 Token 消耗减少 <strong>24%</strong>。<code>Sonnet 4.6</code> 现已上线 <code>API</code> 及各类AI应用，免费版 <strong>Claude</strong> 也可体验<code>Sonnet 4.6</code>。官方建议，对于大规模代码重构等超复杂任务，<code>Opus 4.6</code> 仍是最佳选择，但对多数任务，<code>Sonnet 4.6</code> 提供了极高性价比。</p>
<p><img src="https://cdn.jsdelivr.net/gh/imjuya/picx-images-hosting@master/imagehub/aidaily/e48a712e-9804-40a3-83ef-a724b8c835d1/37ae8237-3741-4ff4-950b-a7944f9f7c68/m001.png" alt="" /></p>
<p><img src="https://cdn.jsdelivr.net/gh/imjuya/picx-images-hosting@master/imagehub/aidaily/e48a712e-9804-40a3-83ef-a724b8c835d1/37ae8237-3741-4ff4-950b-a7944f9f7c68/m002.png" alt="" /></p>
<p><img src="https://cdn.jsdelivr.net/gh/imjuya/picx-images-hosting@master/imagehub/aidaily/e48a712e-9804-40a3-83ef-a724b8c835d1/37ae8237-3741-4ff4-950b-a7944f9f7c68/m003.png" alt="" /></p>
<p>相关链接：</p>
<ul>
<li><a href="https://www.anthropic.com/news/claude-sonnet-4-6">https://www.anthropic.com/news/claude-sonnet-4-6</a></li>
<li><a href="https://claude.com/blog/improved-web-search-with-dynamic-filtering">https://claude.com/blog/improved-web-search-with-dynamic-filtering</a></li>
</ul>
<hr />
<h2><a href="https://x.com/elonmusk/status/2023828048580387001">xAI 上线 Grok 4.20 测试版</a> <code>#2</code></h2>
<blockquote>
<p><strong>xAI</strong> 上线了 <code>Grok 4.20</code> 公开测试版，该版本引入了由四个 <code>Agent</code> 组成的 <code>原生协作系统</code>，用于处理复杂查询。据 <strong>Elon Musk</strong> 称，该版本基于 <strong>5000 亿</strong>参数的 <code>V8</code> 模型，凭借快速学习与每周迭代，<strong>下个月</strong>测试结束时，其智能水平和速度预计将比 <code>Grok 4</code> 提升约一个数量级。</p>
</blockquote>
<p><strong>xAI</strong>上线了<code>Grok 4.20</code>公开测试版，用户需在应用内手动选择。据创始人<strong>Elon Musk</strong>透露，该模型并非单纯迭代，而是基于<strong>500B</strong>参数的<code>V8</code>小型基础模型构建。官方声明指出，<code>Grok 4.2</code>基础设施支持快速学习与每周更新，以实现“<code>递归智能增长</code>”。官方预计，在下个月测试版结束时，其智能水平和速度将比<code>Grok 4</code>提升约一个数量级。</p>
<p>该版本引入的原生<code>多Agent协作系统</code>是其核心亮点。据了解，该系统包含<code>Grok/Captain</code>、<code>Harper</code>、<code>Benjamin</code>和<code>Lucas</code> <strong>四个</strong> Agent，在处理复杂查询时自动运行。</p>
<p><img src="https://cdn.jsdelivr.net/gh/imjuya/picx-images-hosting@master/imagehub/aidaily/e48a712e-9804-40a3-83ef-a724b8c835d1/7078bb72-ab29-4eeb-9a0a-1c0a2666c81d/m001.png" alt="" /></p>
<p><img src="https://cdn.jsdelivr.net/gh/imjuya/picx-images-hosting@master/imagehub/aidaily/e48a712e-9804-40a3-83ef-a724b8c835d1/7078bb72-ab29-4eeb-9a0a-1c0a2666c81d/m002.png" alt="" /></p>
<p><img src="https://cdn.jsdelivr.net/gh/imjuya/picx-images-hosting@master/imagehub/aidaily/e48a712e-9804-40a3-83ef-a724b8c835d1/7078bb72-ab29-4eeb-9a0a-1c0a2666c81d/m003.png" alt="" /></p>
<p><img src="https://cdn.jsdelivr.net/gh/imjuya/picx-images-hosting@master/imagehub/aidaily/e48a712e-9804-40a3-83ef-a724b8c835d1/7078bb72-ab29-4eeb-9a0a-1c0a2666c81d/m004.png" alt="" /></p>
<p>相关链接：</p>
<ul>
<li><a href="https://x.com/elonmusk/status/2023828048580387001">https://x.com/elonmusk/status/2023828048580387001</a></li>
</ul>
<hr />
<h2><a href="https://x.com/NotebookLM/status/2023851190102986970">NotebookLM推出幻灯片Prompt修订与PPTX导出</a> <code>#3</code></h2>
<blockquote>
<p><strong>NotebookLM</strong> 发布重要更新，现在可以直接输入提示词来微调和修改幻灯片内容。同时，系统新增了 <code>PPTX</code> 导出支持，允许用户将生成的演示文稿直接下载为 <code>PPTX</code> 文件。这两项功能目前正在向 <strong>Ultra</strong> 和 <strong>Pro</strong> 会员推送。</p>
</blockquote>
<p><code>NotebookLM</code> 发布两项重要更新：<code>Prompt-Based Revisions</code> 与 <code>PPTX Support</code>，以回应用户强烈需求。</p>
<p>核心功能 <code>Prompt-Based Revisions</code> 允许用户通过 <code>Prompt</code> 描述直接对幻灯片进行调整、定制和微调。此外，<code>NotebookLM</code> 现已支持将生成的幻灯片导出为 <code>PPTX</code> 格式，官方透露 <strong>Google Slides</strong> 的支持即将推出。<code>NotebookLM</code> 正为 <code>Ultra</code> 和 <code>Pro</code> 会员推送这两项新功能：</p>
<p><img src="https://cdn.jsdelivr.net/gh/imjuya/picx-images-hosting@master/imagehub/aidaily/e48a712e-9804-40a3-83ef-a724b8c835d1/a726e5fc-b654-42ea-9d19-ca3d11dc9ace/m001.png" alt="" /></p>
<p>相关链接：</p>
<ul>
<li><a href="https://x.com/NotebookLM/status/2023851190102986970">https://x.com/NotebookLM/status/2023851190102986970</a></li>
</ul>
<hr />
<h2><a href="https://xqacmer.github.io/Ming-Flash-Omni-V2-TTS/">蚂蚁集团开源Ming-omni-tts音频生成模型</a> <code>#4</code></h2>
<blockquote>
<p><strong>蚂蚁集团</strong> <code>Inclusion AI</code> 开源了统一音频生成模型 <code>Ming-Omni-TTS</code>。该模型不仅能生成语音，还能合成音乐和环境音，包含 <strong>0.5B</strong> 和 <strong>16.8B-A3B</strong> 两个版本。</p>
</blockquote>
<p><strong>蚂蚁集团</strong> <strong>inclusionAI</strong> 开源统一音频生成模型 <code>Ming-omni-tts</code>，提供 <strong>0.5B</strong> 及 <code>16.8B-A3B</code> 两个版本。该模型是业界首个在单通道内联合生成语音、环境音和音乐的 <code>自回归模型</code>，通过定制 <strong>12.5Hz</strong> 连续 <code>Tokenizer</code> 实现了 <strong>3.1Hz</strong> 的高效推理帧率。官方评测显示，<code>Ming-omni-tts-16.8B-A3B</code> 在粤语生成、情感控制及零样本语音克隆等基准测试中达到 <code>SOTA</code> 水平，其文本规范化能力媲美 <code>Gemini-2.5 Pro</code>。模型权重及推理代码已上线 <strong>Hugging Face</strong>、<strong>ModelScope</strong> 及 <strong>GitHub</strong>。</p>
<p><img src="https://cdn.jsdelivr.net/gh/imjuya/picx-images-hosting@master/imagehub/aidaily/e48a712e-9804-40a3-83ef-a724b8c835d1/ea284cf1-f5d6-42cf-a715-7f1e71cee91c/m001.png" alt="" /></p>
<p>相关链接：</p>
<ul>
<li><a href="https://xqacmer.github.io/Ming-Flash-Omni-V2-TTS/">https://xqacmer.github.io/Ming-Flash-Omni-V2-TTS/</a></li>
<li><a href="https://github.com/inclusionAI/Ming-omni-tts">https://github.com/inclusionAI/Ming-omni-tts</a></li>
<li><a href="https://modelscope.cn/studios/antsipan/ming-uniaudio-demo">https://modelscope.cn/studios/antsipan/ming-uniaudio-demo</a></li>
</ul>
<hr />
<h2><a href="https://cohere.com/blog/cohere-labs-tiny-aya">Cohere Labs发布Tiny Aya多语言模型</a> <code>#5</code></h2>
<blockquote>
<p><strong>Cohere Labs</strong> 发布了名为 <code>Tiny Aya</code> 的多语言小型模型家族。该系列拥有 <strong>33.5 亿</strong> 参数，覆盖全球 <strong>70 多种</strong> 语言。</p>
</blockquote>
<p><strong>Cohere Labs</strong> 发布多语言小型模型家族 <code>Tiny Aya</code>。该系列包含 <strong>3.35B</strong> 参数基座模型及 <strong>4</strong> 个针对全球及特定区域（南亚、西亚/非洲、欧亚）优化的指令微调模型，覆盖 <strong>70+</strong> 种语言，侧重低资源语言支持。模型上下文 <strong>8K</strong>，采用 <code>CC-BY-NC</code> 协议，支持在笔记本电脑及手机端离线运行。官方指出模型擅长翻译与摘要，但在思维链推理任务上表现较弱。目前模型已在 <strong>Hugging Face</strong>、<strong>Kaggle</strong> 等平台开源，提供 <code>GGUF</code> 格式。</p>
<p><img src="https://cdn.jsdelivr.net/gh/imjuya/picx-images-hosting@master/imagehub/aidaily/e48a712e-9804-40a3-83ef-a724b8c835d1/88832ab7-c4cc-48f2-9407-70a7fc40e493/m001.png" alt="" /></p>
<p>相关链接：</p>
<ul>
<li><a href="https://cohere.com/blog/cohere-labs-tiny-aya">https://cohere.com/blog/cohere-labs-tiny-aya</a></li>
<li><a href="https://github.com/Cohere-Labs/tiny-aya-tech-report/blob/main/tiny_aya_tech_report.pdf">https://github.com/Cohere-Labs/tiny-aya-tech-report/blob/main/tiny_aya_tech_report.pdf</a></li>
<li><a href="https://huggingface.co/collections/CohereLabs/tiny-aya">https://huggingface.co/collections/CohereLabs/tiny-aya</a></li>
</ul>
<hr />
<h2><a href="https://github.com/shallowdream204/BitDance">字节跳动研究团队开源 BitDance 多模态模型</a> <code>#6</code></h2>
<blockquote>
<p><strong>字节跳动</strong>研究团队发布了名为 <code>BitDance</code> 的开源多模态模型，参数量达 <strong>140 亿</strong>，该模型专为视觉生成优化，通过 <code>并行预测 Token</code>，推理速度比标准模型提升超过 <strong>30 倍</strong>。</p>
</blockquote>
<p><strong>字节跳动</strong>研究团队近日发布开源离散自回归多模态模型 <code>BitDance</code>，参数量为 <strong>14B</strong>。模型引入<code>大词汇量二元分词器</code>及<code>下一块扩散范式</code>，支持每步并行预测最多 <strong>64</strong> 个 <code>Token</code>，官方数据显示其比标准 <code>AR 模型</code>推理速度快 <strong>30 倍</strong>以上。</p>
<p>官方发布了 <code>BitDance-14B-64x</code> 和 <code>16x</code> 两个版本，配套 <code>UniWeTok</code> 分词器。在性能方面，<code>BitDance</code> 在 <code>DPG-Bench</code>（<strong>88.28</strong> 分）和 <code>GenEval</code>（<strong>0.86</strong> 分）上表现优异。目前，该模型代码与权重已在 <strong>GitHub</strong> 和 <strong>Hugging Face</strong> 开源（<code>Apache 2.0</code>），并提供在线演示，相关论文已发布于 <strong>arXiv</strong>。</p>
<p><img src="https://cdn.jsdelivr.net/gh/imjuya/picx-images-hosting@master/imagehub/aidaily/e48a712e-9804-40a3-83ef-a724b8c835d1/a5632f31-cd75-4f9a-b20b-abc61940866e/m001.png" alt="" /></p>
<p>相关链接：</p>
<ul>
<li><a href="https://github.com/shallowdream204/BitDance">https://github.com/shallowdream204/BitDance</a></li>
<li><a href="https://bitdance.csuhan.com/">https://bitdance.csuhan.com/</a></li>
<li><a href="https://huggingface.co/collections/shallowdream204/bitdance">https://huggingface.co/collections/shallowdream204/bitdance</a></li>
</ul>
<hr />
<h2><a href="https://cursor.com/changelog/2-5">Cursor 发布 2.5 版本更新，推出插件市场</a> <code>#7</code></h2>
<blockquote>
<p><strong>Cursor</strong> 发布了 <strong>2.5</strong> 版本更新，上线了 <strong>Cursor Marketplace</strong> 插件市场。首批整合了 <strong>Figma</strong>、<strong>Stripe</strong> 和 <strong>AWS</strong> 等工具。此外，<code>子智能体</code>现在支持<code>异步运行</code>与<code>树状协作</code>，<code>沙箱功能</code>新增了<code>细粒度访问控制</code>。</p>
</blockquote>
<p>近日，代码编辑器 <strong>Cursor</strong> 正式发布 <code>2.5</code> 版本，上线了 <code>Cursor Marketplace</code> 插件市场，并对核心 <code>Agent</code> 功能与 <code>沙盒</code> 安全机制进行了升级。</p>
<p>在扩展性方面，新版本引入统一<code>插件</code>机制，将 <code>Skills</code>、<code>Subagents</code>、<code>MCP servers</code> 等能力打包。<code>Cursor Marketplace</code> 已汇集 <strong>Linear</strong>、<strong>Figma</strong>、<strong>Stripe</strong>、<strong>AWS</strong> 等首批合作伙伴插件，覆盖设计、支付、部署及数据分析全流程。用户可通过网页或编辑器内 <code>/add-plugin</code> 命令直接安装。官方已开放插件提交入口，并发布了其内部 <code>CI</code> 和 <code>代码审查</code> 工作流模板 <code>Cursor Team Kit</code>，未来将推出支持统一治理的私有团队插件市场。</p>
<p>在 <code>Agent</code> 性能方面，<code>子智能体</code> 现已支持异步运行与树状层级协作，使<code>父智能体</code>可在后台执行任务，以更低的延迟处理大型重构或多文件任务。基于此，官方推出了具备自主规划与执行能力的 <code>长期运行智能体</code>，官方称在测试中已能生成更完整的 <code>PR</code> 并减少后续干预。</p>
<p>在安全与权限控制方面，<code>沙盒</code> 新增了对域名和本地文件系统的细粒度访问控制，提供 <code>仅用户配置</code>、<code>用户配置+默认值</code> 及 <code>允许全部</code> 三种模式。企业版管理员可通过 <code>管理控制台</code> 强制实施网络策略，确保组织级的出站访问安全。</p>
<p><img src="https://cdn.jsdelivr.net/gh/imjuya/picx-images-hosting@master/imagehub/aidaily/e48a712e-9804-40a3-83ef-a724b8c835d1/be845020-a5ba-43a5-a15f-fac997938c77/m001.png" alt="" /></p>
<p><img src="https://cdn.jsdelivr.net/gh/imjuya/picx-images-hosting@master/imagehub/aidaily/e48a712e-9804-40a3-83ef-a724b8c835d1/be845020-a5ba-43a5-a15f-fac997938c77/m002.png" alt="" /></p>
<p>相关链接：</p>
<ul>
<li><a href="https://cursor.com/changelog/2-5">https://cursor.com/changelog/2-5</a></li>
<li><a href="https://cursor.com/blog/marketplace">https://cursor.com/blog/marketplace</a></li>
</ul>
<hr />
<h2><a href="https://developers.openai.com/codex/concepts/cyber-safety">OpenAI修复GPT-5.3-Codex请求重定向问题</a> <code>#8</code></h2>
<blockquote>
<p>针对部分用户使用 <code>GPT-5.3-Codex</code> 却被路由至 <code>GPT-5.2</code> 的问题，<strong>OpenAI</strong> 称已修复相关 <code>Bug</code> 并校准了 <code>分类器</code>，同时在 <code>CLI</code> <strong>v0.102.0</strong> 版本中加入了显眼的降级通知功能。</p>
</blockquote>
<p><strong>OpenAI</strong> 将 <code>GPT-5.3-Codex</code> 定义为其 <strong>Preparedness Framework</strong> 下的首个**“高网络安全能力”**模型。鉴于网络能力具备支持防御性研究与潜在恶意滥用的双重用途属性，<strong>OpenAI</strong> 实施了包括<code>安全训练</code>和<code>自动监控</code>在内的多重防护措施，会将检测到的<code>可疑网络活动流量</code> <code>重路由</code>至网络能力较弱的 <code>GPT-5.2</code> 模型。</p>
<p>针对近期用户遭遇请求被意外降级的情况，<strong>OpenAI</strong> 团队成员承认，系统曾在特定时段出现<code>过度标记问题</code>，影响了约 <strong>9%</strong> 的用户。该问题已修复，团队通过<code>校准分类器</code>将预期受影响用户比例降至 <strong>1%</strong> 以下，并修复了<code>信任访问权限</code>未生效的 <code>Bug</code>。为提升透明度，<code>CLI v0.102.0</code> 版本已加入请求被降级时的<code>显眼通知</code>，并将在未来几天内扩展至所有客户端。</p>
<p><img src="https://cdn.jsdelivr.net/gh/imjuya/picx-images-hosting@master/imagehub/aidaily/e48a712e-9804-40a3-83ef-a724b8c835d1/01abe632-e2e8-4802-9ff8-8b94bfbd5b27/m001.png" alt="" /></p>
<p>相关链接：</p>
<ul>
<li><a href="https://developers.openai.com/codex/concepts/cyber-safety">https://developers.openai.com/codex/concepts/cyber-safety</a></li>
<li><a href="https://x.com/embirico/status/2023891414623592653">https://x.com/embirico/status/2023891414623592653</a></li>
</ul>
<hr />
<h2><a href="https://inference-docs.cerebras.ai/models/overview">Cerebras下调部分免费层级的推理额度</a> <code>#9</code></h2>
<blockquote>
<p><strong>Cerebras</strong> 官方宣布，由于部分模型需求量激增，已暂时下调相关模型免费层级的 <code>速率限制</code>。</p>
</blockquote>
<p><strong>Cerebras</strong>官方宣布，因<code>zai-glm-4.7</code>和<code>qwen-3-235b-a22b-instruct-2507</code>模型需求激增，已暂时下调免费层级<code>速率限制</code>，正致力恢复原有设置。</p>
<p><img src="https://cdn.jsdelivr.net/gh/imjuya/picx-images-hosting@master/imagehub/aidaily/e48a712e-9804-40a3-83ef-a724b8c835d1/c0ebb726-a3e0-4afc-8ae0-a410a7df87ea/m001.png" alt="" /></p>
<p>相关链接：</p>
<ul>
<li><a href="https://inference-docs.cerebras.ai/models/overview">https://inference-docs.cerebras.ai/models/overview</a></li>
</ul>
<hr />
<h2><a href="https://github.com/Intelligent-Internet/CommonGround">Intelligent Internet 开源多Agent协作系统 Common Ground Core</a> <code>#10</code></h2>
<blockquote>
<p><strong>Intelligent Internet</strong> 宣布开源 <code>多 Agent</code> 协作操作系统 <code>Common Ground Core</code>，这是一个协议优先的 <code>OS</code> 内核，旨在解决 <code>多 Agent 系统</code> 常见的上下文丢失等问题。</p>
</blockquote>
<p><strong>Intelligent Internet</strong> 团队近日开源 <code>多 Agent 协作操作系统</code> <strong>Common Ground Core (CGC)</strong>。该系统定位为 <code>协议优先的 OS 内核</code>，旨在解决 <code>多 Agent</code> 扩展时的 <code>上下文丢失</code>、<code>死锁</code> 及 <code>协调崩溃</code> 等问题。<strong>CGC</strong> 采用 <code>边缘自由、内核约束</code> 设计，利用 <strong>Postgres</strong> 维护 <code>不可变共享认知账本</code> 作为 <code>真理源</code>，通过 <strong>NATS</strong> 消除 <code>分布式消息重排序风险</code>。系统将人类视为与 AI 平等的 <code>异步节点</code>，支持介入协作。目前项目已在 <strong>GitHub</strong> 发布 <code>预览版</code>，提供 <strong>Docker</strong> 部署并集成 <strong>CardBox</strong> 状态模型。官方特别提示，当前版本 <code>API</code> 无认证且具备 <code>任意命令执行能力</code>，严禁直接暴露于 <code>公网</code>。</p>
<p><img src="https://cdn.jsdelivr.net/gh/imjuya/picx-images-hosting@master/imagehub/aidaily/e48a712e-9804-40a3-83ef-a724b8c835d1/b928c84e-5235-405d-819a-51c14e1ad9d8/m001.png" alt="" /></p>
<p>相关链接：</p>
<ul>
<li><a href="https://github.com/Intelligent-Internet/CommonGround">https://github.com/Intelligent-Internet/CommonGround</a></li>
<li><a href="https://ii.inc/web/blog/post/common-ground-core-cgc">https://ii.inc/web/blog/post/common-ground-core-cgc</a></li>
</ul>
<hr />
<h2><a href="https://www.usenerve.com/blog/joining-openai">Nerve加入OpenAI构建ChatGPT搜索</a> <code>#11</code></h2>
<blockquote>
<p>初创公司 <strong>Nerve</strong> 宣布加入 <strong>OpenAI</strong>，团队将致力于在更大规模上为 <strong>ChatGPT</strong> 构建搜索功能。</p>
</blockquote>
<p>企业级 <code>AI Agent</code> 初创公司 <strong>Nerve</strong> 官方宣布加入 <strong>OpenAI</strong>，旨在为 <code>ChatGPT</code> 构建更大规模的搜索功能。<strong>Nerve</strong> 过去 <strong>两年</strong> 专注于以搜索为核心的企业级 <code>Agent</code>，因认可 <strong>OpenAI</strong> 在 <code>信息检索</code> 领域的深度与雄心而决定加入。针对现有客户，<strong>Nerve</strong> 宣布产品将在 <strong>30 天后</strong> 正式关停，即日起暂停所有计费；未来 <strong>30 天内</strong> 服务将继续运行并提供支持，过渡期结束后将安全删除所有客户数据。</p>
<p><img src="https://cdn.jsdelivr.net/gh/imjuya/picx-images-hosting@master/imagehub/aidaily/e48a712e-9804-40a3-83ef-a724b8c835d1/970fab9f-d63e-4427-96e6-c64530e0cae8/m001.png" alt="" /></p>
<p>相关链接：</p>
<ul>
<li><a href="https://www.usenerve.com/blog/joining-openai">https://www.usenerve.com/blog/joining-openai</a></li>
</ul>
<hr />
<h2><a href="https://mp.weixin.qq.com/s/MRx63AOgKZcn9ug8aSiH6w">传 Moonshot AI 完成7亿美元融资</a> <code>#12</code></h2>
<blockquote>
<p>据媒体报道，<strong>月之暗面</strong>完成<strong>7亿美元</strong>融资，<strong>阿里巴巴</strong>和<strong>腾讯</strong>参与投资，公司投后估值超过<strong>100亿美元</strong>。</p>
</blockquote>
<p>据媒体报道，<strong>Moonshot AI（月之暗面）<strong>完成</strong>7亿美元</strong>融资，投后估值超<strong>100亿美元</strong>。本轮融资由<strong>Alibaba</strong>与<strong>Tencent</strong>参与。</p>
<p>相关链接：</p>
<ul>
<li><a href="https://mp.weixin.qq.com/s/MRx63AOgKZcn9ug8aSiH6w">https://mp.weixin.qq.com/s/MRx63AOgKZcn9ug8aSiH6w</a></li>
</ul>
<hr />
<p><strong>提示</strong>：内容由AI辅助创作，可能存在<strong>幻觉</strong>和<strong>错误</strong>。</p>
<p>作者<code>橘鸦Juya</code>，视频版在同名<strong>哔哩哔哩</strong>。欢迎<strong>点赞、关注、分享</strong>。</p>
]]></content><link href="https://github.com/imjuya/gitblog/issues/5"/><published>2026-02-18T09:12:03+00:00</published></entry><entry><id>https://github.com/imjuya/gitblog/issues/4</id><title>Qwen 3.5系列模型即将发布【AI 早报 2026-02-09】</title><updated>2026-02-21T06:50:52.310851+00:00</updated><content type="html"><![CDATA[<p><img src="http://testtttt.oss-cn-guangzhou.aliyuncs.com/imagehub/20260209/20260209084949647072fdba_cover_95db.jpg" alt="" /></p>
<h1>AI 早报 2026-02-09</h1>
<h2>概览</h2>
<h3>精选</h3>
<ul>
<li>Qwen 3.5系列模型即将发布 <code>#1</code></li>
</ul>
<h3>模型发布</h3>
<ul>
<li>xAI上线Grok Imagine Image Pro 模型 API <code>#2</code></li>
</ul>
<h3>产品应用</h3>
<ul>
<li>CodeBuddy团队发布AI桌面工作台WorkBuddy <code>#3</code></li>
</ul>
<h3>前瞻与传闻</h3>
<ul>
<li>字节即将推出Seedream 5.0预览版 <code>#4</code></li>
<li>Meta AI 已在官网测试最新模型 Avocado <code>#5</code></li>
</ul>
<hr />
<h2>Qwen 3.5系列模型即将发布 <code>#1</code></h2>
<blockquote>
<p><strong>千问团队</strong>正在推进 <strong>Qwen 3.5</strong> 系列模型的发布，其已向 <code>Transformers</code> 代码库提交相关支持 <code>PR</code>。该系列模型采用混合架构，结合 <code>Gated DeltaNet</code> 与混合注意力机制，原生支持多模态输入。</p>
</blockquote>
<p>千问团队正在推进 <strong>Qwen 3.5</strong> 系列模型的发布，其 <strong>已向 Hugging Face Transformers 代码库提交相关支持 PR</strong>。该系列采用混合架构，结合 <code>Gated DeltaNet</code> 与 <code>混合注意力机制</code>，原生支持文本、图像及视频处理。代码 PR 确认了至少两个版本的模型存在：<code>Qwen3.5-9B-Instruct</code> 和 <code>Qwen3.5-35B-A3B-Instruct</code>。</p>
<p><img src="https://cdn.jsdelivr.net/gh/imjuya/picx-images-hosting@master/imagehub/aidaily/95db0821-8e25-40a2-8645-bfffdc0798d4/24a0f928-1269-40e2-9c2e-055d3b21daa1/m001.png" alt="" /></p>
<pre><code>https://github.com/huggingface/transformers/pull/43830/
</code></pre>
<hr />
<h2>xAI上线Grok Imagine Image Pro 模型 API <code>#2</code></h2>
<blockquote>
<p>xAI 近日正式宣布推出名为 <strong>Grok Imagine Image Pro</strong> 与 <strong>Grok Imagine</strong> 的图像生成模型，支持文本生图、图像编辑、风格迁移及批量生成。用户可通过 API 调用这两款模型。</p>
</blockquote>
<p>xAI 正式宣布推出名为 <strong>Grok Imagine Image Pro</strong> 与 <strong>Grok Imagine Image</strong> 的两款图像生成模型。模型支持 <strong>文本生成</strong>、<strong>自然语言编辑</strong>、<strong>多轮迭代</strong>、<strong>风格迁移</strong> 以及 <strong>批量生成</strong>，用户可通过 <code>API</code> 访问。<code>API</code> 调用定价为 <strong>Pro 版 $0.07/图</strong>，<strong>标准版 $0.02/图</strong>。</p>
<p><img src="https://cdn.jsdelivr.net/gh/imjuya/picx-images-hosting@master/imagehub/aidaily/95db0821-8e25-40a2-8645-bfffdc0798d4/80399f4c-0d03-4bbf-8f50-9a1de16e9ba5/m001.png" alt="" /></p>
<p><img src="https://cdn.jsdelivr.net/gh/imjuya/picx-images-hosting@master/imagehub/aidaily/95db0821-8e25-40a2-8645-bfffdc0798d4/80399f4c-0d03-4bbf-8f50-9a1de16e9ba5/m002.png" alt="" /></p>
<pre><code>https://docs.x.ai/developers/model-capabilities/images/generation
https://x.com/xai/status/2020313728802242673
</code></pre>
<hr />
<h2>CodeBuddy团队发布AI桌面工作台WorkBuddy <code>#3</code></h2>
<blockquote>
<p><strong>腾讯云CodeBuddy团队</strong>发布<code>WorkBuddy</code>，支持自然语言指令执行多模态任务。产品已开放内测，适用于文件处理、PPT生成、海报设计、知识库构建等场景。</p>
</blockquote>
<p><strong>腾讯云 CodeBuddy</strong> 团队近日发布了 <code>WorkBuddy</code>，一款定位为全场景职场 AI 智能体的桌面工作台，目前已开放内测申请。该产品支持通过自然语言下达任务，可在本地电脑自主规划并执行多模态复杂任务，核心功能包括对经授权的本地文件进行批量处理、生成文档/表格/PPT、开展数据分析及行业调研。</p>
<p><img src="https://cdn.jsdelivr.net/gh/imjuya/picx-images-hosting@master/imagehub/aidaily/95db0821-8e25-40a2-8645-bfffdc0798d4/31b0c065-fbc6-424d-b846-cf0b6c9b6e01/m001.png" alt="" /></p>
<pre><code>https://mp.weixin.qq.com/s/jepj-IFv3UR9LpZlt-h82w
https://www.codebuddy.cn/work
</code></pre>
<hr />
<h2>字节即将推出Seedream 5.0预览版 <code>#4</code></h2>
<blockquote>
<p>字节即将推出 <strong>Seedream 5.0</strong> 预览版，相关文档已发布，该模型支持联网检索、精准编辑与智能推理，首发 <code>2k</code>/<code>4k</code> 高清生图且限时免费。</p>
</blockquote>
<p><strong>字节跳动</strong>的图像生成模型 <code>Seedream（即梦）</code> 即将推出 <code>5.0-Preview</code> 版本，提供<strong>联网实时检索</strong>、<strong>编辑精准可控</strong>及<strong>智能逻辑推理</strong>三大核心功能，并首发<strong>限免2k与4k清晰度</strong>。但官方文档称该预览版在真实感与美感上存在效果劣化，建议对生成效果有严苛需求的用户使用 <code>4.5</code> 版本或等待<strong>年后</strong>发布的 <code>5.0</code> 正式版。</p>
<p><code>5.0-Preview</code> 首次支持<strong>检索生图</strong>功能，旨在将创作与热门资讯深度融合。检索开关为条件性触发，时效词或长尾词可高概率激活。在<strong>编辑精准可控</strong>方面，新版本提升了生成图像与输入文本的契合度。<strong>智能逻辑推理</strong>功能则涵盖复杂逻辑推演、物理世界知识与垂类行业知识。模型不仅能对花朵按品种分类构图，还能理解物理规律和参考 <code>CAD</code> 设计图生成真实建筑。</p>
<p><img src="https://cdn.jsdelivr.net/gh/imjuya/picx-images-hosting@master/imagehub/20260209/20260209082129_6a016a2537.png" alt="" /></p>
<pre><code>https://bytedance.larkoffice.com/wiki/TQyJwfRFiiVMUdkFY84cCb7CncI
</code></pre>
<hr />
<h2>Meta AI 已在官网测试最新模型 Avocado <code>#5</code></h2>
<blockquote>
<p>有用户发现，<strong>Meta AI</strong> 更新了其网站与移动应用，正在测试包括新模型 <code>Avocado</code> 在内的多项功能与集成。</p>
</blockquote>
<p>Meta AI 更新了其网站与移动应用，并正在测试包括新模型 <code>Avocado</code> 在内的多项功能与集成。Meta 的内部测试模式包括 <code>Avocado</code>、<code>Avocado Thinking</code> 和代表 browser agent 的 <code>Sierra</code> 模型。<code>Avocado</code> 是继 <code>Llama 4</code> 之后 Meta 准备发布的模型的代号。另一个内部概念为“Big Brain”，其思路是让多个模型 agents 并行运行，并从中选择最佳输出作为最终响应。</p>
<p>目前，<code>Avocado</code>、voice agent、<code>browser agent</code>（<code>Sierra</code>）和“Tasks”等模型和功能尚在测试或开发中，尚未完成最终部署。</p>
<p><img src="https://cdn.jsdelivr.net/gh/imjuya/picx-images-hosting@master/imagehub/aidaily/95db0821-8e25-40a2-8645-bfffdc0798d4/7b128843-d679-4af6-826d-5c2ab7936564/m001.gif" alt="" /></p>
<pre><code>https://www.testingcatalog.com/meta-ai-redies-avacado-manus-agent-and-openclaw-integration/
</code></pre>
<hr />
<p><strong>提示</strong>：内容由AI辅助创作，可能存在<strong>幻觉</strong>和<strong>错误</strong>。</p>
<p>作者<code>橘鸦Juya</code>，视频版在同名<strong>哔哩哔哩</strong>。欢迎<strong>点赞、关注、分享</strong>。</p>
]]></content><link href="https://github.com/imjuya/gitblog/issues/4"/><published>2026-02-09T03:59:29+00:00</published></entry><entry><id>https://github.com/imjuya/gitblog/issues/3</id><title>test2</title><updated>2026-02-21T06:50:52.437273+00:00</updated><content type="html"><![CDATA[<video width="560" height="315" controls>
    <source src="https://www.example.com/video.mp4" type="video/mp4">
    Your browser does not support the video tag.
</video>
<h1>橘鸦 AI 早报 2025-08-01</h1>
<h2>概览</h2>
<ul>
<li>阶跃星辰正式开源321B多模态模型Step3</li>
<li>千问发布Qwen3-Coder-30B-A3B-Instruct</li>
<li>Black Forest Labs与Krea合作发布图像模型FLUX.1 Krea [dev]</li>
<li>Cohere发布专为企业设计的112B多模态模型Command A Vision</li>
<li>字节跳动发布实验性扩散语言模型Seed Diffusion Preview</li>
<li>Veo 3 Fast 即将登陆 Google AI Studio</li>
<li>Quora推出Poe API，提供对上百种AI模型的统一访问</li>
<li>Gemini CLI更新，支持自定义斜杠命令及多项功能改进</li>
<li>Google开源LangExtract：从非结构化文本中提取结构化信息的Python库</li>
<li>Vercel发布AI SDK 5，全面革新全栈AI应用开发</li>
<li>OpenAI在挪威启动Stargate数据中心项目</li>
<li>国务院常务会议部署深入实施“人工智能+”行动</li>
<li>AI行业动态与研究简讯</li>
</ul>
<h2>阶跃星辰正式开源321B多模态模型Step3</h2>
<blockquote>
<p><strong>阶跃星辰</strong>开源了其<strong>321B</strong>参数的<code>MoE</code>多模态大模型<code>Step3</code>，旨在通过创新的架构为企业提供高性价比的推理方案。</p>
</blockquote>
<p><strong>阶跃星辰</strong>宣布正式开源其最新一代基础大模型<code>Step3</code>。该模型采用专家混合（<code>MoE</code>）架构，总参数量为<strong>321B</strong>，激活参数量为<strong>38B</strong>，旨在为企业和开发者提供性能与成本极致均衡的推理方案。</p>
<p><code>Step3</code>模型在设计上专注于多模态推理，通过端到端的设计最小化解码成本，在视觉语言推理任务中表现出色。技术上，模型采用了自研的<code>MFA</code>（Multi-matrix Factorization Attention）注意力机制和<code>AFD</code>（Attention-FFN Disaggregation）系统架构。<code>MFA</code>旨在降低KV缓存开销和计算消耗，而<code>AFD</code>则将Attention和FFN计算解耦为两个子系统，通过流水线并行调度提升吞吐效率。为支持<code>AFD</code>，<strong>阶跃星辰</strong>还开源了专用的通信库<code>StepMesh</code>，以实现跨卡的低延迟高带宽数据传输。</p>
<p>在性能评测方面，<code>Step3</code>在<strong>MMMU</strong>、<strong>MathVision</strong>、<strong>AIME 2025</strong>等多个基准上，表现优于同类开源模型。在社区测试中，该模型也展现了不错的指令遵循和生成能力。<strong>vLLM</strong>项目宣布已支持<code>Step3</code>模型，并报告在<strong>Hopper GPU</strong>上实现了高达<strong>4,039 tok/sec/GPU</strong>的吞吐量。</p>
<p><code>Step3</code>模型权重已在<strong>Hugging Face</strong>和<strong>魔搭社区</strong>发布，支持<code>bf16</code>和<code>block-fp8</code>格式。用户可以通过<strong>阶跃星辰开放平台</strong>访问其<code>OpenAI</code>兼容的API，上下文长度为<strong>64K</strong>。目前提供折扣价格，具体如下：</p>
<div style="text-align:center;">
<table>
<thead>
<tr>
<th align="left">项目</th>
<th align="left">价格（每百万token）</th>
</tr>
</thead>
<tbody>
<tr>
<td align="left">输入</td>
<td align="left"><strong>1.5元</strong></td>
</tr>
<tr>
<td align="left">输出</td>
<td align="left"><strong>4元</strong></td>
</tr>
</tbody></table></div>
<img src="https://static.stepfun.com/static/studio-doc/7dfddc5d-2bf0-495d-a843-ce30a9788e4e_1753967248524.jpeg" alt="Step3模型性能图" style="max-width:100%;height:auto;display:block;margin:16px 0;" />
<pre><code>https://mp.weixin.qq.com/s/RKsSTgbzP1A-xC8ADmZ2kw
https://huggingface.co/stepfun-ai/step3
https://github.com/stepfun-ai/Step3
</code></pre>
<h2>千问发布Qwen3-Coder-30B-A3B-Instruct</h2>
<blockquote>
<p><strong>千问团队</strong>发布了采用<code>MoE</code>架构的<strong>30.5B</strong>编码模型<code>Qwen3-Coder-30B-A3B-Instruct</code>，该模型支持<strong>256K</strong>超长上下文并针对<code>Agentic</code>任务进行了优化。</p>
</blockquote>
<p><strong>千问团队</strong>发布了<code>Qwen3-Coder</code>系列的新模型——<code>Qwen3-Coder-30B-A3B-Instruct</code>。这是一款精简但性能强大的编码模型，采用专家混合（<code>MoE</code>）架构，总参数量为<strong>30.5B</strong>，激活参数量为<strong>3.3B</strong>。</p>
<p>该模型在多个方面进行了关键增强。其在<code>Agentic</code>编码和<code>Agentic</code>浏览器使用等任务上表现出色，并支持为<code>Qwen Code</code>、<code>CLINE</code>等平台设计的特定函数调用格式。模型原生支持<strong>256K tokens</strong>的超长上下文，并可通过<code>Yarn</code>技术扩展至<strong>1M tokens</strong>，专为仓库级别的代码理解进行了优化。</p>
<p>模型架构细节包括<strong>48</strong>个层、<strong>32</strong>个Q注意力头和<strong>4</strong>个KV注意力头，以及<strong>128</strong>个专家和<strong>8</strong>个激活专家。官方指出，新模型仅支持非思考模式。</p>
<p><strong>Hugging Face</strong>页面提供了详细的快速上手代码示例，并建议在遇到内存不足问题时，可将上下文长度缩短至<strong>32,768</strong>。该模型也已获得<strong>Ollama</strong>、<strong>LMStudio</strong>、<strong>MLX-LM</strong>等本地应用的支持。</p>
<img src="https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen3-Coder/qwen3-coder-30a3-main.jpg" alt="Qwen3-Coder模型" style="max-width:100%;height:auto;display:block;margin:16px 0;" />
<pre><code>https://huggingface.co/Qwen/Qwen3-Coder-30B-A3B-Instruct
</code></pre>
<h2>Black Forest Labs与Krea合作发布图像模型FLUX.1 Krea [dev]</h2>
<blockquote>
<p><strong>Black Forest Labs</strong>与<strong>Krea</strong>合作发布了<strong>120亿</strong>参数的开放权重图像模型<code>FLUX.1 Krea [dev]</code>，该模型采用“修正流”技术，旨在生成具有独特美学和真实感的图像。</p>
</blockquote>
<p><strong>Black Forest Labs</strong>与<strong>Krea</strong>合作，发布了一款名为<code>FLUX.1 Krea [dev]</code>的开放权重模型。这是一个<strong>120亿</strong>参数的“修正流”（<code>rectified flow</code>）Transformer模型，专为生成具有独特美学和卓越真实感的摄影级图像而设计。</p>
<p>该模型是<strong>Krea 1</strong>的开放权重版本，经过训练可以生成更加真实和多样化的图像，避免了过度饱和的纹理和常见的“AI感”。它通过指导蒸馏（<code>guidance distillation</code>）技术进行训练，提高了效率。官方称该模型为“有主见”的文生图模型，旨在为用户提供视觉上有趣的惊喜。</p>
<p><code>FLUX.1 Krea [dev]</code>的权重已在<strong>Hugging Face</strong>上发布，并已集成到<strong>ComfyUI</strong>和<strong>Diffusers</strong>中。用户可以在<strong>ComfyUI</strong>中直接下载<code>safetensors</code>文件使用，或通过<strong>Diffusers</strong>的<code>FluxPipeline</code>进行调用。模型生成的输出可用于个人、科研和商业目的，但需遵守<code>FLUX.1 [dev]</code>非商业许可协议（<code>FluxDev Non-Commercial License Agreement</code>）和可接受使用政策。</p>
<p>在安全方面，<strong>Black Forest Labs</strong>和<strong>Krea</strong>在发布前采取了多项措施，包括对预训练数据进行<code>NSFW</code>内容过滤，与<strong>互联网观察基金会</strong>（<strong>IWF</strong>）合作过滤已知的<code>CSAM</code>（儿童性虐待材料），以及进行多轮微调以抑制滥用。模型在对抗性测试中表现出高弹性，并包含了推理过滤器以防止生成非法内容。</p>
<img src="https://pbs.twimg.com/amplify_video_thumb/1950920435035508739/img/aDfDCqXD4UQ30rnI.jpg" alt="FLUX.1 Krea [dev]生成图像示例" style="max-width:100%;height:auto;display:block;margin:16px 0;" />
<pre><code>https://huggingface.co/black-forest-labs/FLUX.1-Krea-dev
https://bfl.ai/announcements/flux-1-krea-dev
</code></pre>
<h2>Cohere发布专为企业设计的112B多模态模型Command A Vision</h2>
<blockquote>
<p><strong>Cohere</strong>发布了专为企业优化的<strong>1120亿</strong>参数多模态模型<code>Command A Vision</code>，该模型在保持低资源占用的同时，提供了卓越的图像理解和多语言处理能力。</p>
</blockquote>
<p><strong>Cohere</strong>发布了一款名为<code>Command A Vision</code>的全新多模态模型，该模型经过优化，旨在为企业提供卓越的图像理解能力，同时保持较低的计算资源占用。作为一个拥有<strong>1120亿</strong>参数的开放权重研究版本，<code>Command A Vision</code>在私有化部署时仅需<strong>两块</strong>或更少的GPU（如<strong>两块A100</strong>或<strong>一块H100</strong>），确保了企业级的可扩展性。</p>
<p>模型架构方面，<code>Command A Vision</code>将基于<code>Command A</code>的语言模型与<strong>Google</strong>的<code>SigLIP2-patch16-512</code>视觉编码器通过一个多模态适配器相结合。它支持<strong>32k</strong>的上下文长度，并能处理<strong>英语</strong>、<strong>葡萄牙语</strong>、<strong>意大利语</strong>、<strong>法语</strong>、<strong>德语</strong>和<strong>西班牙语</strong>等多种语言。在图像处理上，该模型使用最多<strong>12个</strong> <strong>512x512</strong>像素的图像块和<strong>1个</strong>缩略图来编码单张图片，总计最多使用<strong>3328个</strong>视觉token，推荐处理最高分辨率为<strong>2048x1536</strong>（<strong>300万像素</strong>）的图像。</p>
<p>在性能上，<strong>Cohere</strong>表示<code>Command A Vision</code>在多个关键多模态基准测试中超越了<strong>GPT-4.1</strong>、<strong>Llama 4 Maverick</strong>和<strong>Mistral Medium 3</strong>等同类模型。它特别擅长处理企业级的视觉任务，包括分析图表、图形、表格和技术图纸；通过<code>OCR</code>技术从发票、表单等文档中准确提取文本和结构化信息；以及理解现实世界的复杂场景，可用于工业风险检测和零售分析等应用。该模型结合了<code>Command A</code>的先进<code>RAG</code>与引用功能，并支持<code>JSON</code>模式以输出结构化数据。</p>
<p><code>Command A Vision</code>目前已在<strong>Cohere平台</strong>和<strong>Hugging Face</strong>上提供，遵循<code>CC-BY-NC</code>非商业许可，并要求遵守<strong>Cohere Lab</strong>的可接受使用政策。对于商业用途，企业需联系<strong>Cohere</strong>的销售团队。</p>
<img src="https://cohere.com/_next/image?url=https%3A%2F%2Fcohere-ai.ghost.io%2Fcontent%2Fimages%2F2025%2F07%2F250730_blog-image_CommandAVision_chart-detailed-benchmarks-2.png&w=3840&q=75" alt="Command A Vision性能对比图" style="max-width:100%;height:auto;display:block;margin:16px 0;" />
<pre><code>https://cohere.com/blog/command-a-vision
https://huggingface.co/CohereLabs/command-a-vision-07-2025
</code></pre>
<h2>字节跳动发布实验性扩散语言模型Seed Diffusion Preview</h2>
<blockquote>
<p><strong>字节跳动Seed团队</strong>发布了实验性扩散语言模型<code>Seed Diffusion Preview</code>，通过多项创新技术探索离散扩散在结构化代码生成领域的可行性，并实现了显著的推理速度提升。</p>
</blockquote>
<p><strong>字节跳动Seed团队</strong>发布了一款名为<code>Seed Diffusion Preview</code>的实验性扩散语言模型，旨在探索离散扩散技术在结构化代码生成领域的可行性。该模型的目标是验证扩散模型作为下一代语言模型基础框架的潜力。</p>
<p>为实现这一目标，团队引入了多项关键技术。首先是“<code>两阶段课程学习</code>”，第一阶段通过标准的掩码填充任务训练模型掌握代码的局部上下文和模式，第二阶段则引入基于编辑距离的插入/删除操作，强制模型评估和修正全局代码的合理性。其次，团队提出了“<code>约束顺序扩散</code>”训练方法，通过从预训练模型合成并筛选偏好的生成轨迹进行蒸馏，让扩散模型学习代码中固有的因果依赖关系。</p>
<p>为了提升解码效率，团队采用了“<code>同策略学习</code>”范式，通过优化一个旨在最小化生成步数同时保证输出质量的代理损失函数，让模型学会更直接、高效地收敛到高质量结果。在工程实现上，团队采用了块级并行扩散采样方案，并利用内部优化的基础设施框架支持高效推理。</p>
<p>实验结果显示，<code>Seed Diffusion Preview</code>的代码推理速度可达<strong>2146 tokens/s</strong>，相比同等规模的自回归（<code>AR</code>）模型提升了<strong>5.4倍</strong>。同时，在多个核心代码基准测试中，其性能与优秀的<code>AR</code>模型相当，甚至在<code>CanItEdit</code>等代码修复任务上实现了超越。</p>
<img src="https://lf3-static.bytednsdoc.com/obj/eden-cn/hyvsmeh7uhobf/img_v3_02om_0ade2e5d-5b89-4ea2-b89d-73954f083d4g.jpg" alt="Seed Diffusion模型架构图" style="max-width:100%;height:auto;display:block;margin:16px 0;" />
<pre><code>https://seed.bytedance.com/zh/seed_diffusion
</code></pre>
<h2>Veo 3 Fast 即将登陆 Google AI Studio</h2>
<blockquote>
<p><strong>Google</strong>宣布推出专为速度和成本效益优化的视频模型<code>Veo 3 Fast</code>，并为<code>Veo 3</code>系列增加了图像到视频生成功能，这些更新已通过<code>Gemini API</code>提供。</p>
</blockquote>
<p><strong>Google</strong>宣布为其视频生成模型系列增添新成员<code>Veo 3 Fast</code>，并为<code>Veo 3</code>和<code>Veo 3 Fast</code>增加了图像到视频的生成能力。这些更新现已通过<code>Gemini API</code>以付费预览形式提供。</p>
<p><code>Veo 3 Fast</code>是<code>Veo 3</code>模型的一个优化版本，专为追求速度和成本效益的开发者设计，使其能够更快地进行创意迭代。该模型支持文本到视频和图像到视频两种模式，非常适用于程序化广告、快速创意原型设计和大规模社交媒体内容生成等场景。</p>
<p>新增的图像到视频功能允许开发者使用<code>Veo 3</code>和<code>Veo 3 Fast</code>从静态输入图像生成带有声音的高质量视频片段。用户只需提供一张图片和相应的文本提示，即可引导模型生成具有期望动作、叙事和音效的动态视频，并能保持与初始图像的风格一致性。</p>
<table>
<thead>
<tr>
<th align="left">模型/功能</th>
<th align="left">定价（每秒视频，含音频）</th>
</tr>
</thead>
<tbody>
<tr>
<td align="left"><code>Veo 3 Fast</code></td>
<td align="left"><strong>0.40美元</strong></td>
</tr>
<tr>
<td align="left"><code>Veo 3</code> (含图像到视频)</td>
<td align="left"><strong>0.75美元</strong></td>
</tr>
</tbody></table><p>此外，有消息称<code>Veo 3 Fast</code>可能很快也会登陆**Google</p>
]]></content><link href="https://github.com/imjuya/gitblog/issues/3"/><category term="THINGS"/><published>2025-08-01T10:34:19+00:00</published></entry><entry><id>https://github.com/imjuya/gitblog/issues/2</id><title>test</title><updated>2026-02-21T06:50:52.571989+00:00</updated><content type="html"><![CDATA[<p>test</p>
<p><img src="https://github.com/user-attachments/assets/91c9e004-f487-4686-8a14-c243a12e6872" alt="Image" /></p>
]]></content><link href="https://github.com/imjuya/gitblog/issues/2"/><published>2025-07-27T10:03:07+00:00</published></entry></feed>